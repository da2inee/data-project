import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.envì— ì ì€ DB ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤)
load_dotenv()

db_user = os.getenv("DB_USER")
db_pw = os.getenv("DB_PASSWORD")
db_host = os.getenv("DB_HOST")
db_port = os.getenv("DB_PORT")
db_name = os.getenv("DB_NAME")

# DB ì—°ê²° ì—”ì§„ ìƒì„±
db_url = f"postgresql://{db_user}:{db_pw}@{db_host}:{db_port}/{db_name}"
engine = create_engine(db_url)

# 2. Extract (ìˆ˜ì§‘): êµ¬ê¸€ ë‰´ìŠ¤ RSS í”¼ë“œ(XML) ê°€ì ¸ì˜¤ê¸°
print("ğŸ“¡ êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
response = requests.get(url)
soup = BeautifulSoup(response.content, "xml") # XML í˜•ì‹ìœ¼ë¡œ ì½ê¸°

# ë‰´ìŠ¤ ì•„ì´í…œ ì¶”ì¶œ (ìµœì‹  10ê°œ)
items = soup.find_all("item")
news_list = []

for item in items[:10]:
    news_list.append({
        "title": item.title.text,
        "link": item.link.text,
        "pub_date": item.pubDate.text
    })

# 3. Transform (ê°€ê³µ): ë°ì´í„°ë¥¼ í‘œ(DataFrame) í˜•íƒœë¡œ ë³€í™˜
df = pd.DataFrame(news_list)

# 4. Load (ì ì¬): ë„ì»¤ë¡œ ì‹¤í–‰ ì¤‘ì¸ PostgreSQLì— ì €ì¥
try:
    # 'daily_news'ë¼ëŠ” í…Œì´ë¸” ì´ë¦„ìœ¼ë¡œ ì €ì¥ (ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
    df.to_sql('daily_news', engine, if_exists='replace', index=False)
    print(f"âœ… ì„±ê³µ! {len(df)}ê°œì˜ ìµœì‹  ë‰´ìŠ¤ê°€ DBì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ ì‹¤íŒ¨: {e}")