"""
ê³ ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ
- ìŠ¤ì¼€ì¤„ë§: ë§¤ì¼ ìë™ ì‹¤í–‰
- ì—ëŸ¬ í•¸ë“¤ë§: ì¬ì‹œë„ ë¡œì§
- ë¡œê¹…: ì‹¤í–‰ ê¸°ë¡ ì €ì¥
- ë°ì´í„° ê²€ì¦: í’ˆì§ˆ ì²´í¬
"""

import os
import time
import logging
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
from apscheduler.schedulers.blocking import BlockingScheduler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class DataPipeline:
    """ETL íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """DB ì—°ê²° ì´ˆê¸°í™”"""
        self.engine = self._create_db_engine()
        self.max_retries = 3
        self.retry_delay = 5  # ì´ˆ
        
    def _create_db_engine(self):
        """DB ì—”ì§„ ìƒì„±"""
        try:
            db_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
            return create_engine(db_url)
        except Exception as e:
            logger.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def extract_news(self) -> List[Dict]:
        """
        Extract: êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘
        ì¬ì‹œë„ ë¡œì§ í¬í•¨
        """
        url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œë„ {attempt + 1}/{self.max_retries}")
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, "xml")
                items = soup.find_all("item")
                
                news_list = []
                for item in items[:20]:  # ìµœì‹  20ê°œ
                    news_list.append({
                        "title": item.title.text,
                        "link": item.link.text,
                        "pub_date": item.pubDate.text,
                        "collected_at": datetime.now()
                    })
                
                logger.info(f"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                return news_list
                
            except Exception as e:
                logger.warning(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return []
        
        return []
    
    def transform_data(self, news_list: List[Dict]) -> pd.DataFrame:
        """
        Transform: ë°ì´í„° ì •ì œ ë° ê°€ê³µ
        """
        if not news_list:
            logger.warning("ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()
        
        try:
            df = pd.DataFrame(news_list)
            
            # 1. ì¤‘ë³µ ì œê±°
            before_count = len(df)
            df = df.drop_duplicates(subset=['title'])
            logger.info(f"ì¤‘ë³µ ì œê±°: {before_count} â†’ {len(df)}ê°œ")
            
            # 2. ì œëª© ê¸¸ì´ ì¶”ê°€
            df['title_length'] = df['title'].str.len()
            
            # 3. ë‚ ì§œ í˜•ì‹ ë³€í™˜
            df['pub_date'] = pd.to_datetime(df['pub_date'])
            
            # 4. ë°ì´í„° ê²€ì¦
            self._validate_data(df)
            
            logger.info("âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return df
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def _validate_data(self, df: pd.DataFrame):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['title', 'link', 'pub_date']
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        
        # ë¹ˆ ê°’ í™•ì¸
        null_counts = df[required_columns].isnull().sum()
        if null_counts.any():
            logger.warning(f"ë¹ˆ ê°’ ë°œê²¬:\n{null_counts[null_counts > 0]}")
        
        logger.info("âœ… ë°ì´í„° ê²€ì¦ í†µê³¼")
    
    def load_to_db(self, df: pd.DataFrame, table_name: str = 'daily_news'):
        """
        Load: ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬
        """
        if df.empty:
            logger.warning("ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        if not self.engine:
            logger.error("DB ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        try:
            # append ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
            df.to_sql(table_name, self.engine, if_exists='append', index=False)
            logger.info(f"âœ… {len(df)}ê°œì˜ ë ˆì½”ë“œë¥¼ '{table_name}' í…Œì´ë¸”ì— ì ì¬ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"DB ì ì¬ ì‹¤íŒ¨: {e}")
            return False
    
    def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=" * 50)
        logger.info("ğŸš€ ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 50)
        
        start_time = time.time()
        
        # Extract
        news_list = self.extract_news()
        
        # Transform
        df = self.transform_data(news_list)
        
        # Load
        success = self.load_to_db(df)
        
        # ì‹¤í–‰ ì‹œê°„
        elapsed_time = time.time() - start_time
        logger.info(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        if success:
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ")
        else:
            logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
        
        logger.info("=" * 50)
        return success


def schedule_pipeline():
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    ë§¤ì¼ ì˜¤ì „ 9ì‹œì™€ ì˜¤í›„ 6ì‹œì— ìë™ ì‹¤í–‰
    """
    scheduler = BlockingScheduler()
    pipeline = DataPipeline()
    
    # ë§¤ì¼ 09:00ì— ì‹¤í–‰
    scheduler.add_job(pipeline.run_pipeline, 'cron', hour=9, minute=0)
    
    # ë§¤ì¼ 18:00ì— ì‹¤í–‰
    scheduler.add_job(pipeline.run_pipeline, 'cron', hour=18, minute=0)
    
    logger.info("ğŸ“… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 09:00, 18:00 ì‹¤í–‰)")
    logger.info("ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
    
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")


if __name__ == "__main__":
    # ë°©ë²• 1: ì¦‰ì‹œ ì‹¤í–‰
    pipeline = DataPipeline()
    pipeline.run_pipeline()
    
    # ë°©ë²• 2: ìŠ¤ì¼€ì¤„ë§ (ì£¼ì„ í•´ì œí•˜ë©´ ìë™ ì‹¤í–‰)
    # schedule_pipeline()
